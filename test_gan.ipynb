{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from train_gan import Critic, Generator, SignalEncoder, SignalDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignalEncoder(\n",
       "  (convnet): Sequential(\n",
       "    (0): Conv1d(1, 8, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Conv1d(8, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Conv1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=960, out_features=100, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formfactor_encoder = SignalEncoder(signal_dims=240, latent_dims=10)\n",
    "formfactor_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0438, -0.0162, -0.1125, -0.0902,  0.0593, -0.0598,  0.0466, -0.1160,\n",
       "         -0.0814, -0.0080],\n",
       "        [ 0.0431, -0.0170, -0.1120, -0.0893,  0.0568, -0.0609,  0.0506, -0.1188,\n",
       "         -0.0793, -0.0091],\n",
       "        [ 0.0423, -0.0146, -0.1116, -0.0892,  0.0576, -0.0607,  0.0489, -0.1160,\n",
       "         -0.0812, -0.0085]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formfactor = torch.rand(3, 240)\n",
    "formfactor_encoder(formfactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignalEncoder(\n",
       "  (convnet): Sequential(\n",
       "    (0): Conv1d(1, 8, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Conv1d(8, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Conv1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=1216, out_features=100, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_encoder = SignalEncoder(signal_dims=300, latent_dims=10)\n",
    "current_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1769,  0.0814,  0.0207, -0.1438, -0.0163,  0.0421, -0.0626,  0.0496,\n",
       "          0.0802,  0.0387],\n",
       "        [-0.1768,  0.0788,  0.0212, -0.1429, -0.0156,  0.0420, -0.0661,  0.0483,\n",
       "          0.0848,  0.0397],\n",
       "        [-0.1780,  0.0818,  0.0193, -0.1457, -0.0157,  0.0389, -0.0666,  0.0471,\n",
       "          0.0812,  0.0402]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_profile = torch.rand(3, 300)\n",
    "current_encoder(current_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignalDecoder(\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=50, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=50, out_features=100, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=100, out_features=1216, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (unflatten): Unflatten(dim=1, unflattened_size=(32, 38))\n",
       "  (convnet): Sequential(\n",
       "    (0): ConvTranspose1d(32, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): ConvTranspose1d(16, 8, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): ConvTranspose1d(8, 1, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "    (5): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_decoder = SignalDecoder(latent_dims=10 + 5 + 1, signal_dims=300)\n",
    "current_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6181, 0.7200, 0.5701, 0.6856, 0.6380, 0.6796, 0.5719, 0.6164, 0.5950,\n",
       "         0.7326, 0.5750, 0.6969, 0.6455, 0.6632, 0.5605, 0.6532, 0.6091, 0.7390,\n",
       "         0.5725, 0.6910, 0.6450, 0.6688, 0.5552, 0.6649, 0.6013, 0.7158, 0.5585,\n",
       "         0.6978, 0.6255, 0.6911, 0.5582, 0.6709, 0.6231, 0.6895, 0.5507, 0.7095,\n",
       "         0.6423, 0.6792, 0.5559, 0.6428, 0.5938, 0.7492, 0.5631, 0.6665, 0.6281,\n",
       "         0.7093, 0.5511, 0.6761, 0.6138, 0.7166, 0.5672, 0.6921, 0.6301, 0.6792,\n",
       "         0.5657, 0.6454, 0.6145, 0.7078, 0.5666, 0.6565, 0.6273, 0.6909, 0.5529,\n",
       "         0.6874, 0.6128, 0.7289, 0.5507, 0.7128, 0.6840, 0.6351, 0.5413, 0.6672,\n",
       "         0.6044, 0.7351, 0.5602, 0.6956, 0.6350, 0.6857, 0.5550, 0.6800, 0.6236,\n",
       "         0.7051, 0.5607, 0.6751, 0.6210, 0.7095, 0.5529, 0.6743, 0.6171, 0.6959,\n",
       "         0.5544, 0.6559, 0.6000, 0.7267, 0.5596, 0.6722, 0.6124, 0.7050, 0.5592,\n",
       "         0.6468, 0.6173, 0.7018, 0.5510, 0.6854, 0.6104, 0.7333, 0.5733, 0.7068,\n",
       "         0.6316, 0.6846, 0.5525, 0.6689, 0.6022, 0.7236, 0.5672, 0.6788, 0.6254,\n",
       "         0.6902, 0.5701, 0.6632, 0.6062, 0.7275, 0.5658, 0.6797, 0.6320, 0.6998,\n",
       "         0.5504, 0.6720, 0.6026, 0.7297, 0.5585, 0.7148, 0.6196, 0.6892, 0.5558,\n",
       "         0.6801, 0.6159, 0.6964, 0.5613, 0.6587, 0.6271, 0.6923, 0.5485, 0.6976,\n",
       "         0.6126, 0.7280, 0.5477, 0.7100, 0.6328, 0.6932, 0.5595, 0.6668, 0.6157,\n",
       "         0.7198, 0.5700, 0.6920, 0.6436, 0.6772, 0.5572, 0.6374, 0.5963, 0.7378,\n",
       "         0.5550, 0.6726, 0.6311, 0.6943, 0.5648, 0.6368, 0.6120, 0.7392, 0.5509,\n",
       "         0.7039, 0.6348, 0.6818, 0.5442, 0.6670, 0.6117, 0.7054, 0.5548, 0.6772,\n",
       "         0.6198, 0.6915, 0.5630, 0.6542, 0.6023, 0.7197, 0.5795, 0.6578, 0.6339,\n",
       "         0.6883, 0.5630, 0.6246, 0.5850, 0.7469, 0.5666, 0.7083, 0.6309, 0.6921,\n",
       "         0.5642, 0.6559, 0.6088, 0.7364, 0.5658, 0.6714, 0.6219, 0.7176, 0.5648,\n",
       "         0.6591, 0.6092, 0.7302, 0.5826, 0.6692, 0.6092, 0.7055, 0.5531, 0.6927,\n",
       "         0.6129, 0.7243, 0.5629, 0.6660, 0.6317, 0.6905, 0.5403, 0.6952, 0.6209,\n",
       "         0.7162, 0.5731, 0.6907, 0.6311, 0.6833, 0.5640, 0.6461, 0.6114, 0.7384,\n",
       "         0.5801, 0.6853, 0.6313, 0.6963, 0.5619, 0.6566, 0.6108, 0.7102, 0.5633,\n",
       "         0.6818, 0.6235, 0.6822, 0.5639, 0.6552, 0.6134, 0.7500, 0.5937, 0.6743,\n",
       "         0.6270, 0.6869, 0.5734, 0.6418, 0.6087, 0.7177, 0.5622, 0.6798, 0.6276,\n",
       "         0.6821, 0.5494, 0.6624, 0.6040, 0.7320, 0.5869, 0.6713, 0.6169, 0.6895,\n",
       "         0.5679, 0.6545, 0.6174, 0.7228, 0.5587, 0.6959, 0.6313, 0.6870, 0.5466,\n",
       "         0.6545, 0.5985, 0.7444, 0.5748, 0.6646, 0.6346, 0.6714, 0.5490, 0.6826,\n",
       "         0.6156, 0.7218, 0.5636, 0.6867, 0.6503, 0.6812, 0.5635, 0.6718, 0.6212,\n",
       "         0.6563, 0.5620, 0.6057],\n",
       "        [0.6205, 0.7214, 0.5698, 0.6856, 0.6435, 0.6734, 0.5680, 0.6137, 0.5906,\n",
       "         0.7392, 0.5762, 0.6953, 0.6375, 0.6721, 0.5640, 0.6484, 0.6087, 0.7351,\n",
       "         0.5774, 0.6952, 0.6455, 0.6696, 0.5580, 0.6558, 0.6026, 0.7187, 0.5611,\n",
       "         0.6941, 0.6262, 0.6863, 0.5555, 0.6726, 0.6177, 0.6975, 0.5566, 0.7111,\n",
       "         0.6395, 0.6809, 0.5553, 0.6458, 0.5997, 0.7441, 0.5603, 0.6622, 0.6261,\n",
       "         0.7146, 0.5524, 0.6750, 0.6129, 0.7190, 0.5694, 0.6853, 0.6253, 0.6876,\n",
       "         0.5652, 0.6437, 0.6158, 0.7061, 0.5660, 0.6601, 0.6296, 0.6850, 0.5555,\n",
       "         0.6864, 0.6117, 0.7308, 0.5511, 0.7042, 0.6759, 0.6430, 0.5402, 0.6763,\n",
       "         0.6078, 0.7272, 0.5635, 0.6876, 0.6341, 0.6851, 0.5576, 0.6749, 0.6234,\n",
       "         0.7039, 0.5605, 0.6760, 0.6186, 0.7111, 0.5553, 0.6698, 0.6172, 0.6895,\n",
       "         0.5498, 0.6548, 0.5988, 0.7306, 0.5597, 0.6749, 0.6120, 0.7070, 0.5590,\n",
       "         0.6479, 0.6220, 0.7019, 0.5548, 0.6758, 0.6101, 0.7329, 0.5725, 0.7088,\n",
       "         0.6300, 0.6890, 0.5561, 0.6612, 0.6001, 0.7292, 0.5704, 0.6807, 0.6213,\n",
       "         0.6946, 0.5649, 0.6654, 0.6087, 0.7261, 0.5697, 0.6694, 0.6267, 0.7079,\n",
       "         0.5572, 0.6566, 0.5985, 0.7296, 0.5550, 0.7214, 0.6224, 0.6861, 0.5588,\n",
       "         0.6757, 0.6149, 0.6959, 0.5610, 0.6545, 0.6237, 0.6977, 0.5497, 0.7000,\n",
       "         0.6159, 0.7259, 0.5487, 0.7113, 0.6336, 0.6872, 0.5565, 0.6770, 0.6157,\n",
       "         0.7189, 0.5700, 0.6921, 0.6461, 0.6763, 0.5573, 0.6421, 0.5983, 0.7350,\n",
       "         0.5573, 0.6698, 0.6304, 0.6973, 0.5631, 0.6413, 0.6105, 0.7409, 0.5557,\n",
       "         0.7066, 0.6331, 0.6745, 0.5447, 0.6634, 0.6073, 0.7065, 0.5555, 0.6699,\n",
       "         0.6178, 0.6980, 0.5659, 0.6521, 0.5990, 0.7176, 0.5757, 0.6626, 0.6209,\n",
       "         0.7043, 0.5647, 0.6308, 0.5900, 0.7381, 0.5635, 0.7026, 0.6299, 0.6957,\n",
       "         0.5647, 0.6552, 0.6116, 0.7300, 0.5620, 0.6774, 0.6215, 0.7180, 0.5613,\n",
       "         0.6607, 0.6081, 0.7289, 0.5795, 0.6757, 0.6070, 0.7113, 0.5551, 0.6923,\n",
       "         0.6125, 0.7166, 0.5620, 0.6726, 0.6323, 0.6850, 0.5392, 0.6954, 0.6223,\n",
       "         0.7117, 0.5665, 0.6880, 0.6245, 0.6958, 0.5642, 0.6531, 0.6132, 0.7395,\n",
       "         0.5774, 0.7021, 0.6346, 0.6933, 0.5629, 0.6508, 0.6094, 0.7132, 0.5657,\n",
       "         0.6765, 0.6210, 0.6854, 0.5645, 0.6530, 0.6099, 0.7521, 0.5955, 0.6704,\n",
       "         0.6262, 0.6919, 0.5715, 0.6471, 0.6103, 0.7129, 0.5627, 0.6742, 0.6297,\n",
       "         0.6840, 0.5484, 0.6616, 0.5996, 0.7388, 0.5883, 0.6958, 0.6192, 0.6869,\n",
       "         0.5670, 0.6487, 0.6122, 0.7268, 0.5616, 0.6938, 0.6316, 0.6877, 0.5494,\n",
       "         0.6504, 0.6008, 0.7451, 0.5763, 0.6620, 0.6314, 0.6709, 0.5506, 0.6832,\n",
       "         0.6157, 0.7226, 0.5620, 0.6870, 0.6490, 0.6794, 0.5624, 0.6746, 0.6211,\n",
       "         0.6575, 0.5645, 0.6029],\n",
       "        [0.6139, 0.7207, 0.5716, 0.6882, 0.6417, 0.6706, 0.5649, 0.6279, 0.5936,\n",
       "         0.7430, 0.5755, 0.6911, 0.6323, 0.6762, 0.5597, 0.6536, 0.6112, 0.7283,\n",
       "         0.5679, 0.6857, 0.6383, 0.6732, 0.5569, 0.6684, 0.6071, 0.7111, 0.5590,\n",
       "         0.6921, 0.6295, 0.6847, 0.5584, 0.6713, 0.6233, 0.6870, 0.5570, 0.6894,\n",
       "         0.6341, 0.6830, 0.5504, 0.6611, 0.6005, 0.7379, 0.5553, 0.6702, 0.6291,\n",
       "         0.7084, 0.5540, 0.6741, 0.6094, 0.7223, 0.5719, 0.6857, 0.6256, 0.6846,\n",
       "         0.5648, 0.6462, 0.6191, 0.7055, 0.5646, 0.6638, 0.6303, 0.6821, 0.5546,\n",
       "         0.6911, 0.6133, 0.7294, 0.5534, 0.7090, 0.6758, 0.6476, 0.5403, 0.6723,\n",
       "         0.6028, 0.7371, 0.5703, 0.6923, 0.6348, 0.6836, 0.5603, 0.6665, 0.6188,\n",
       "         0.7069, 0.5604, 0.6777, 0.6150, 0.7203, 0.5575, 0.6751, 0.6185, 0.6899,\n",
       "         0.5523, 0.6640, 0.5988, 0.7330, 0.5624, 0.6757, 0.6102, 0.7075, 0.5613,\n",
       "         0.6462, 0.6168, 0.7008, 0.5509, 0.6837, 0.6087, 0.7321, 0.5736, 0.7084,\n",
       "         0.6295, 0.6835, 0.5574, 0.6666, 0.6043, 0.7215, 0.5657, 0.6795, 0.6283,\n",
       "         0.6867, 0.5658, 0.6602, 0.6067, 0.7245, 0.5641, 0.6751, 0.6300, 0.6971,\n",
       "         0.5538, 0.6646, 0.5987, 0.7242, 0.5549, 0.7105, 0.6175, 0.6922, 0.5563,\n",
       "         0.6749, 0.6122, 0.7044, 0.5579, 0.6630, 0.6219, 0.7005, 0.5526, 0.6964,\n",
       "         0.6134, 0.7324, 0.5530, 0.7170, 0.6425, 0.6789, 0.5525, 0.6776, 0.6204,\n",
       "         0.7122, 0.5724, 0.6861, 0.6430, 0.6723, 0.5562, 0.6482, 0.5990, 0.7343,\n",
       "         0.5549, 0.6790, 0.6307, 0.6970, 0.5662, 0.6394, 0.6150, 0.7325, 0.5545,\n",
       "         0.6966, 0.6253, 0.6890, 0.5501, 0.6652, 0.6147, 0.7033, 0.5573, 0.6636,\n",
       "         0.6129, 0.7052, 0.5724, 0.6400, 0.6022, 0.7137, 0.5739, 0.6499, 0.6231,\n",
       "         0.6987, 0.5617, 0.6361, 0.5906, 0.7376, 0.5679, 0.6944, 0.6248, 0.7009,\n",
       "         0.5637, 0.6605, 0.6122, 0.7279, 0.5644, 0.6690, 0.6206, 0.7210, 0.5616,\n",
       "         0.6594, 0.6116, 0.7157, 0.5714, 0.6689, 0.6003, 0.7191, 0.5591, 0.7000,\n",
       "         0.6207, 0.7118, 0.5534, 0.6860, 0.6369, 0.6837, 0.5390, 0.6892, 0.6176,\n",
       "         0.7176, 0.5759, 0.6730, 0.6225, 0.6923, 0.5638, 0.6572, 0.6078, 0.7356,\n",
       "         0.5684, 0.7020, 0.6225, 0.7137, 0.5653, 0.6523, 0.6119, 0.7040, 0.5605,\n",
       "         0.6734, 0.6064, 0.7074, 0.5650, 0.6576, 0.6154, 0.7330, 0.5857, 0.6813,\n",
       "         0.6275, 0.6802, 0.5696, 0.6489, 0.6135, 0.7099, 0.5590, 0.6764, 0.6353,\n",
       "         0.6757, 0.5471, 0.6672, 0.6069, 0.7293, 0.5859, 0.6748, 0.6166, 0.6943,\n",
       "         0.5655, 0.6584, 0.6176, 0.7204, 0.5580, 0.6764, 0.6161, 0.7062, 0.5460,\n",
       "         0.6658, 0.6029, 0.7377, 0.5700, 0.6723, 0.6258, 0.6868, 0.5522, 0.6776,\n",
       "         0.6190, 0.7192, 0.5590, 0.7010, 0.6543, 0.6690, 0.5584, 0.6860, 0.6256,\n",
       "         0.6509, 0.5631, 0.6042]], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent = torch.rand(3, 10 + 5 + 1)\n",
    "current_decoder(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignalDecoder(\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=50, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=50, out_features=100, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=100, out_features=960, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (unflatten): Unflatten(dim=1, unflattened_size=(32, 30))\n",
       "  (convnet): Sequential(\n",
       "    (0): ConvTranspose1d(32, 16, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): ConvTranspose1d(16, 8, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): ConvTranspose1d(8, 1, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "    (5): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formfactor_decoder = SignalDecoder(latent_dims=10 + 5 + 1, signal_dims=240)\n",
    "formfactor_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2512, 0.1607, 0.3342, 0.0848, 0.2348, 0.1481, 0.3468, 0.0869, 0.2471,\n",
       "         0.1549, 0.3385, 0.0831, 0.2337, 0.1543, 0.3418, 0.0926, 0.2449, 0.1559,\n",
       "         0.3568, 0.0771, 0.2459, 0.1656, 0.3543, 0.0957, 0.2607, 0.1558, 0.3337,\n",
       "         0.0987, 0.2476, 0.1473, 0.3460, 0.0757, 0.2517, 0.1682, 0.3491, 0.0820,\n",
       "         0.2268, 0.1459, 0.3407, 0.0923, 0.2608, 0.1700, 0.3373, 0.0978, 0.2413,\n",
       "         0.1538, 0.3529, 0.0736, 0.2485, 0.1675, 0.3671, 0.0856, 0.2614, 0.1443,\n",
       "         0.3354, 0.0832, 0.2509, 0.1609, 0.3479, 0.0882, 0.2424, 0.1474, 0.3425,\n",
       "         0.0807, 0.2532, 0.1684, 0.3307, 0.0990, 0.2393, 0.1411, 0.3533, 0.0683,\n",
       "         0.2482, 0.1656, 0.3477, 0.0942, 0.2213, 0.1480, 0.3376, 0.0825, 0.2530,\n",
       "         0.1542, 0.3335, 0.0841, 0.2333, 0.1688, 0.3583, 0.0950, 0.2592, 0.1626,\n",
       "         0.3366, 0.0984, 0.2548, 0.1478, 0.3353, 0.0886, 0.2482, 0.1569, 0.3366,\n",
       "         0.1020, 0.2585, 0.1530, 0.3432, 0.0778, 0.2538, 0.1646, 0.3395, 0.0988,\n",
       "         0.2401, 0.1406, 0.3335, 0.0783, 0.2396, 0.1721, 0.3565, 0.0865, 0.2413,\n",
       "         0.1546, 0.3440, 0.0853, 0.2495, 0.1654, 0.3372, 0.0892, 0.2490, 0.1474,\n",
       "         0.3469, 0.0876, 0.2578, 0.1640, 0.3366, 0.0902, 0.2355, 0.1499, 0.3376,\n",
       "         0.0987, 0.2643, 0.1678, 0.3247, 0.1014, 0.2494, 0.1506, 0.3438, 0.0829,\n",
       "         0.2593, 0.1684, 0.3567, 0.0844, 0.2341, 0.1481, 0.3407, 0.0854, 0.2460,\n",
       "         0.1680, 0.3541, 0.0940, 0.2460, 0.1576, 0.3513, 0.0917, 0.2668, 0.1594,\n",
       "         0.3240, 0.0947, 0.2413, 0.1429, 0.3420, 0.0893, 0.2531, 0.1677, 0.3473,\n",
       "         0.0894, 0.2447, 0.1478, 0.3283, 0.0932, 0.2685, 0.1589, 0.3284, 0.0958,\n",
       "         0.2515, 0.1636, 0.3488, 0.0900, 0.2531, 0.1553, 0.3294, 0.0848, 0.2366,\n",
       "         0.1434, 0.3519, 0.0792, 0.2554, 0.1651, 0.3399, 0.0921, 0.2352, 0.1495,\n",
       "         0.3467, 0.0765, 0.2582, 0.1717, 0.3627, 0.0957, 0.2429, 0.1354, 0.3475,\n",
       "         0.0704, 0.2571, 0.1632, 0.3420, 0.0870, 0.2269, 0.1634, 0.3372, 0.0949,\n",
       "         0.2513, 0.1692, 0.3263, 0.1059, 0.2437, 0.1535, 0.3254, 0.1061, 0.2633,\n",
       "         0.1601, 0.3296, 0.0933, 0.2488, 0.1394, 0.3484, 0.0726, 0.2475, 0.1612,\n",
       "         0.3429, 0.0870, 0.2353, 0.1400, 0.3232, 0.0269],\n",
       "        [0.2526, 0.1621, 0.3318, 0.0899, 0.2441, 0.1461, 0.3502, 0.0812, 0.2421,\n",
       "         0.1566, 0.3583, 0.0732, 0.2272, 0.1556, 0.3463, 0.0927, 0.2468, 0.1564,\n",
       "         0.3581, 0.0795, 0.2434, 0.1655, 0.3649, 0.0909, 0.2585, 0.1541, 0.3387,\n",
       "         0.0975, 0.2526, 0.1409, 0.3331, 0.0847, 0.2502, 0.1683, 0.3418, 0.0772,\n",
       "         0.2250, 0.1504, 0.3451, 0.0941, 0.2616, 0.1771, 0.3414, 0.0966, 0.2445,\n",
       "         0.1564, 0.3585, 0.0734, 0.2473, 0.1689, 0.3622, 0.0869, 0.2555, 0.1436,\n",
       "         0.3310, 0.0847, 0.2516, 0.1588, 0.3499, 0.0863, 0.2397, 0.1484, 0.3496,\n",
       "         0.0804, 0.2530, 0.1609, 0.3224, 0.1032, 0.2445, 0.1280, 0.3434, 0.0672,\n",
       "         0.2476, 0.1620, 0.3391, 0.0967, 0.2270, 0.1482, 0.3436, 0.0791, 0.2540,\n",
       "         0.1588, 0.3389, 0.0815, 0.2331, 0.1677, 0.3660, 0.0932, 0.2649, 0.1575,\n",
       "         0.3383, 0.0994, 0.2567, 0.1440, 0.3295, 0.0877, 0.2481, 0.1579, 0.3354,\n",
       "         0.1083, 0.2625, 0.1575, 0.3468, 0.0751, 0.2556, 0.1676, 0.3410, 0.0971,\n",
       "         0.2403, 0.1366, 0.3423, 0.0742, 0.2472, 0.1747, 0.3618, 0.0837, 0.2370,\n",
       "         0.1538, 0.3510, 0.0810, 0.2482, 0.1666, 0.3392, 0.0899, 0.2460, 0.1456,\n",
       "         0.3418, 0.0906, 0.2582, 0.1634, 0.3273, 0.0969, 0.2439, 0.1502, 0.3397,\n",
       "         0.0977, 0.2666, 0.1614, 0.3329, 0.0943, 0.2430, 0.1373, 0.3473, 0.0726,\n",
       "         0.2603, 0.1642, 0.3609, 0.0765, 0.2336, 0.1568, 0.3482, 0.0907, 0.2541,\n",
       "         0.1632, 0.3558, 0.0894, 0.2409, 0.1555, 0.3497, 0.0892, 0.2691, 0.1616,\n",
       "         0.3376, 0.0934, 0.2426, 0.1370, 0.3426, 0.0834, 0.2515, 0.1689, 0.3531,\n",
       "         0.0863, 0.2439, 0.1459, 0.3358, 0.0898, 0.2650, 0.1556, 0.3296, 0.0921,\n",
       "         0.2537, 0.1603, 0.3569, 0.0867, 0.2552, 0.1521, 0.3260, 0.0836, 0.2334,\n",
       "         0.1413, 0.3515, 0.0732, 0.2529, 0.1672, 0.3461, 0.0976, 0.2318, 0.1500,\n",
       "         0.3491, 0.0702, 0.2589, 0.1750, 0.3784, 0.0846, 0.2316, 0.1419, 0.3591,\n",
       "         0.0675, 0.2563, 0.1659, 0.3418, 0.0902, 0.2244, 0.1525, 0.3339, 0.0948,\n",
       "         0.2523, 0.1677, 0.3285, 0.1007, 0.2464, 0.1534, 0.3328, 0.1044, 0.2590,\n",
       "         0.1568, 0.3306, 0.0865, 0.2466, 0.1479, 0.3489, 0.0739, 0.2463, 0.1582,\n",
       "         0.3397, 0.0875, 0.2379, 0.1423, 0.3223, 0.0273],\n",
       "        [0.2532, 0.1583, 0.3274, 0.0882, 0.2376, 0.1505, 0.3465, 0.0878, 0.2428,\n",
       "         0.1540, 0.3432, 0.0736, 0.2281, 0.1557, 0.3468, 0.0928, 0.2467, 0.1604,\n",
       "         0.3574, 0.0824, 0.2429, 0.1678, 0.3646, 0.0891, 0.2616, 0.1557, 0.3374,\n",
       "         0.0946, 0.2436, 0.1463, 0.3494, 0.0746, 0.2500, 0.1684, 0.3498, 0.0752,\n",
       "         0.2225, 0.1458, 0.3396, 0.0967, 0.2658, 0.1682, 0.3327, 0.0955, 0.2415,\n",
       "         0.1636, 0.3515, 0.0771, 0.2509, 0.1702, 0.3685, 0.0847, 0.2583, 0.1438,\n",
       "         0.3390, 0.0832, 0.2510, 0.1608, 0.3477, 0.0878, 0.2366, 0.1470, 0.3423,\n",
       "         0.0822, 0.2542, 0.1637, 0.3272, 0.1011, 0.2428, 0.1435, 0.3475, 0.0734,\n",
       "         0.2535, 0.1655, 0.3350, 0.1026, 0.2252, 0.1390, 0.3310, 0.0778, 0.2538,\n",
       "         0.1571, 0.3367, 0.0865, 0.2351, 0.1666, 0.3624, 0.0931, 0.2596, 0.1628,\n",
       "         0.3382, 0.1006, 0.2577, 0.1486, 0.3347, 0.0882, 0.2433, 0.1548, 0.3385,\n",
       "         0.1027, 0.2590, 0.1555, 0.3435, 0.0773, 0.2523, 0.1685, 0.3430, 0.0975,\n",
       "         0.2395, 0.1395, 0.3385, 0.0737, 0.2389, 0.1765, 0.3629, 0.0889, 0.2444,\n",
       "         0.1516, 0.3475, 0.0803, 0.2501, 0.1704, 0.3413, 0.0884, 0.2406, 0.1440,\n",
       "         0.3505, 0.0838, 0.2568, 0.1632, 0.3399, 0.0884, 0.2365, 0.1437, 0.3439,\n",
       "         0.0911, 0.2663, 0.1640, 0.3252, 0.0994, 0.2409, 0.1483, 0.3425, 0.0836,\n",
       "         0.2620, 0.1698, 0.3596, 0.0834, 0.2386, 0.1506, 0.3413, 0.0859, 0.2458,\n",
       "         0.1635, 0.3492, 0.0850, 0.2416, 0.1558, 0.3521, 0.0919, 0.2695, 0.1582,\n",
       "         0.3238, 0.0979, 0.2391, 0.1422, 0.3403, 0.0855, 0.2504, 0.1675, 0.3498,\n",
       "         0.0901, 0.2449, 0.1495, 0.3282, 0.0935, 0.2663, 0.1561, 0.3268, 0.0924,\n",
       "         0.2474, 0.1650, 0.3478, 0.0934, 0.2571, 0.1588, 0.3299, 0.0889, 0.2414,\n",
       "         0.1447, 0.3529, 0.0824, 0.2584, 0.1631, 0.3341, 0.0964, 0.2355, 0.1544,\n",
       "         0.3404, 0.0809, 0.2588, 0.1724, 0.3671, 0.0877, 0.2433, 0.1435, 0.3508,\n",
       "         0.0698, 0.2525, 0.1683, 0.3399, 0.0979, 0.2304, 0.1546, 0.3229, 0.0975,\n",
       "         0.2509, 0.1641, 0.3303, 0.0997, 0.2435, 0.1588, 0.3353, 0.1062, 0.2611,\n",
       "         0.1584, 0.3273, 0.0967, 0.2482, 0.1378, 0.3485, 0.0681, 0.2464, 0.1630,\n",
       "         0.3476, 0.0861, 0.2284, 0.1364, 0.3248, 0.0248]],\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent = torch.rand(3, 10 + 5 + 1)\n",
    "formfactor_decoder(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (formfactor_encoder): SignalEncoder(\n",
       "    (convnet): Sequential(\n",
       "      (0): Conv1d(1, 8, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Conv1d(8, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Conv1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=960, out_features=100, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=50, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (current_decoder): SignalDecoder(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=50, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=50, out_features=100, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=100, out_features=1216, bias=True)\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (unflatten): Unflatten(dim=1, unflattened_size=(32, 38))\n",
       "    (convnet): Sequential(\n",
       "      (0): ConvTranspose1d(32, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): ConvTranspose1d(16, 8, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): ConvTranspose1d(8, 1, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator()\n",
    "generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4286, 0.4041, 0.4640, 0.4182, 0.4252, 0.3907, 0.4733, 0.4247, 0.4205,\n",
       "         0.4137, 0.4620, 0.4118, 0.4276, 0.3913, 0.4752, 0.4194, 0.4279, 0.3908,\n",
       "         0.4734, 0.4186, 0.4273, 0.3981, 0.4709, 0.4155, 0.4306, 0.4057, 0.4614,\n",
       "         0.4034, 0.4317, 0.3921, 0.4684, 0.4106, 0.4309, 0.4040, 0.4646, 0.4002,\n",
       "         0.4297, 0.3754, 0.4690, 0.4005, 0.4324, 0.3877, 0.4725, 0.3993, 0.4339,\n",
       "         0.3889, 0.4709, 0.4117, 0.4280, 0.4076, 0.4720, 0.4189, 0.4254, 0.3840,\n",
       "         0.4731, 0.4087, 0.4301, 0.4307, 0.4594, 0.4077, 0.4243, 0.3763, 0.4773,\n",
       "         0.4168, 0.4295, 0.3970, 0.4637, 0.4156, 0.4219, 0.3945, 0.4728, 0.4141,\n",
       "         0.4276, 0.4161, 0.4641, 0.4149, 0.4191, 0.3799, 0.4640, 0.4012, 0.4288,\n",
       "         0.4071, 0.4642, 0.4177, 0.4281, 0.3855, 0.4727, 0.4219, 0.4260, 0.4167,\n",
       "         0.4595, 0.4105, 0.4244, 0.3838, 0.4604, 0.4076, 0.4221, 0.4044, 0.4656,\n",
       "         0.4051, 0.4301, 0.3910, 0.4753, 0.4159, 0.4258, 0.4116, 0.4628, 0.4013,\n",
       "         0.4258, 0.3803, 0.4756, 0.4121, 0.4300, 0.3958, 0.4637, 0.4031, 0.4342,\n",
       "         0.3886, 0.4784, 0.4178, 0.4292, 0.4147, 0.4619, 0.4141, 0.4258, 0.3844,\n",
       "         0.4711, 0.4146, 0.4240, 0.4051, 0.4589, 0.3980, 0.4339, 0.3858, 0.4697,\n",
       "         0.4078, 0.4264, 0.4109, 0.4655, 0.4170, 0.4250, 0.4016, 0.4706, 0.4199,\n",
       "         0.4257, 0.4059, 0.4645, 0.4145, 0.4206, 0.3829, 0.4712, 0.4064, 0.4312,\n",
       "         0.4139, 0.4642, 0.4172, 0.4255, 0.3919, 0.4766, 0.4117, 0.4326, 0.4133,\n",
       "         0.4573, 0.4179, 0.4208, 0.3884, 0.4745, 0.3958, 0.4314, 0.3988, 0.4637,\n",
       "         0.4192, 0.4241, 0.3858, 0.4724, 0.4173, 0.4250, 0.4017, 0.4559, 0.3925,\n",
       "         0.4313, 0.3849, 0.4717, 0.4073, 0.4277, 0.4071, 0.4638, 0.4103, 0.4289,\n",
       "         0.3942, 0.4768, 0.4281, 0.4228, 0.4083, 0.4577, 0.4144, 0.4270, 0.4047,\n",
       "         0.4677, 0.4117, 0.4277, 0.3906, 0.4646, 0.4059, 0.4305, 0.3982, 0.4679,\n",
       "         0.4114, 0.4224, 0.4145, 0.4650, 0.4100, 0.4269, 0.3943, 0.4758, 0.4166,\n",
       "         0.4220, 0.4021, 0.4672, 0.4161, 0.4290, 0.4054, 0.4729, 0.4169, 0.4306,\n",
       "         0.4058, 0.4623, 0.4238, 0.4203, 0.4019, 0.4636, 0.4083, 0.4267, 0.4016,\n",
       "         0.4696, 0.4212, 0.4298, 0.3969, 0.4690, 0.4145, 0.4267, 0.4143, 0.4597,\n",
       "         0.4113, 0.4237, 0.3879, 0.4703, 0.4049, 0.4282, 0.4127, 0.4683, 0.4225,\n",
       "         0.4224, 0.3975, 0.4737, 0.4165, 0.4294, 0.4149, 0.4633, 0.4134, 0.4203,\n",
       "         0.3812, 0.4795, 0.4135, 0.4281, 0.4151, 0.4681, 0.4193, 0.4268, 0.3844,\n",
       "         0.4824, 0.4298, 0.4233, 0.4161, 0.4650, 0.4147, 0.4263, 0.3745, 0.4789,\n",
       "         0.4064, 0.4283, 0.4090, 0.4622, 0.4267, 0.4224, 0.3940, 0.4700, 0.4134,\n",
       "         0.4231, 0.4190, 0.4598, 0.4082, 0.4244, 0.4020, 0.4775, 0.4188, 0.4277,\n",
       "         0.3891, 0.4635, 0.3790],\n",
       "        [0.4285, 0.4020, 0.4638, 0.4189, 0.4258, 0.3910, 0.4734, 0.4228, 0.4196,\n",
       "         0.4124, 0.4631, 0.4141, 0.4271, 0.3930, 0.4733, 0.4184, 0.4266, 0.3903,\n",
       "         0.4731, 0.4150, 0.4289, 0.3971, 0.4720, 0.4156, 0.4309, 0.4052, 0.4614,\n",
       "         0.4025, 0.4316, 0.3918, 0.4693, 0.4109, 0.4312, 0.4029, 0.4643, 0.4005,\n",
       "         0.4283, 0.3773, 0.4682, 0.3986, 0.4337, 0.3890, 0.4738, 0.4000, 0.4347,\n",
       "         0.3904, 0.4710, 0.4124, 0.4280, 0.4092, 0.4717, 0.4201, 0.4246, 0.3828,\n",
       "         0.4743, 0.4078, 0.4296, 0.4311, 0.4582, 0.4087, 0.4241, 0.3777, 0.4777,\n",
       "         0.4171, 0.4293, 0.3973, 0.4634, 0.4146, 0.4216, 0.3926, 0.4722, 0.4124,\n",
       "         0.4286, 0.4170, 0.4636, 0.4147, 0.4187, 0.3792, 0.4640, 0.3988, 0.4287,\n",
       "         0.4045, 0.4650, 0.4182, 0.4289, 0.3881, 0.4730, 0.4206, 0.4260, 0.4196,\n",
       "         0.4602, 0.4124, 0.4227, 0.3823, 0.4627, 0.4085, 0.4215, 0.4052, 0.4656,\n",
       "         0.4044, 0.4300, 0.3860, 0.4758, 0.4151, 0.4262, 0.4119, 0.4628, 0.4002,\n",
       "         0.4268, 0.3820, 0.4765, 0.4130, 0.4300, 0.3951, 0.4635, 0.4033, 0.4334,\n",
       "         0.3854, 0.4786, 0.4181, 0.4297, 0.4159, 0.4625, 0.4158, 0.4249, 0.3836,\n",
       "         0.4704, 0.4132, 0.4238, 0.4053, 0.4598, 0.3996, 0.4326, 0.3882, 0.4694,\n",
       "         0.4062, 0.4270, 0.4098, 0.4660, 0.4180, 0.4254, 0.4023, 0.4707, 0.4197,\n",
       "         0.4255, 0.4074, 0.4651, 0.4148, 0.4184, 0.3814, 0.4718, 0.4066, 0.4302,\n",
       "         0.4149, 0.4651, 0.4190, 0.4239, 0.3899, 0.4763, 0.4109, 0.4320, 0.4126,\n",
       "         0.4578, 0.4156, 0.4203, 0.3878, 0.4761, 0.3989, 0.4307, 0.3992, 0.4633,\n",
       "         0.4179, 0.4251, 0.3864, 0.4705, 0.4146, 0.4253, 0.4032, 0.4571, 0.3952,\n",
       "         0.4318, 0.3872, 0.4715, 0.4065, 0.4276, 0.4078, 0.4632, 0.4101, 0.4276,\n",
       "         0.3902, 0.4794, 0.4276, 0.4219, 0.4073, 0.4575, 0.4157, 0.4258, 0.4017,\n",
       "         0.4683, 0.4130, 0.4274, 0.3919, 0.4656, 0.4068, 0.4317, 0.4001, 0.4679,\n",
       "         0.4131, 0.4229, 0.4122, 0.4654, 0.4104, 0.4270, 0.3950, 0.4758, 0.4160,\n",
       "         0.4232, 0.4032, 0.4678, 0.4202, 0.4284, 0.4073, 0.4731, 0.4172, 0.4300,\n",
       "         0.4051, 0.4602, 0.4203, 0.4207, 0.4001, 0.4658, 0.4104, 0.4260, 0.4030,\n",
       "         0.4696, 0.4196, 0.4285, 0.3962, 0.4670, 0.4119, 0.4275, 0.4152, 0.4613,\n",
       "         0.4113, 0.4239, 0.3882, 0.4696, 0.4047, 0.4286, 0.4111, 0.4689, 0.4221,\n",
       "         0.4231, 0.3977, 0.4728, 0.4166, 0.4295, 0.4148, 0.4638, 0.4118, 0.4213,\n",
       "         0.3823, 0.4794, 0.4134, 0.4277, 0.4145, 0.4691, 0.4203, 0.4278, 0.3871,\n",
       "         0.4794, 0.4293, 0.4237, 0.4165, 0.4662, 0.4140, 0.4270, 0.3743, 0.4793,\n",
       "         0.4088, 0.4266, 0.4054, 0.4641, 0.4287, 0.4237, 0.3959, 0.4685, 0.4106,\n",
       "         0.4232, 0.4172, 0.4598, 0.4076, 0.4246, 0.4021, 0.4780, 0.4201, 0.4271,\n",
       "         0.3897, 0.4638, 0.3784],\n",
       "        [0.4278, 0.4048, 0.4641, 0.4183, 0.4252, 0.3904, 0.4745, 0.4234, 0.4202,\n",
       "         0.4138, 0.4621, 0.4144, 0.4276, 0.3941, 0.4747, 0.4200, 0.4266, 0.3907,\n",
       "         0.4728, 0.4177, 0.4272, 0.3975, 0.4691, 0.4107, 0.4316, 0.4043, 0.4616,\n",
       "         0.4047, 0.4311, 0.3936, 0.4668, 0.4052, 0.4310, 0.4021, 0.4646, 0.4024,\n",
       "         0.4289, 0.3768, 0.4689, 0.3978, 0.4326, 0.3878, 0.4728, 0.4024, 0.4330,\n",
       "         0.3892, 0.4713, 0.4117, 0.4290, 0.4102, 0.4686, 0.4165, 0.4248, 0.3809,\n",
       "         0.4737, 0.4084, 0.4295, 0.4251, 0.4596, 0.4046, 0.4252, 0.3780, 0.4761,\n",
       "         0.4144, 0.4301, 0.3996, 0.4631, 0.4179, 0.4207, 0.3945, 0.4696, 0.4115,\n",
       "         0.4277, 0.4172, 0.4650, 0.4194, 0.4178, 0.3798, 0.4617, 0.3984, 0.4288,\n",
       "         0.4037, 0.4649, 0.4162, 0.4298, 0.3875, 0.4750, 0.4201, 0.4258, 0.4152,\n",
       "         0.4595, 0.4103, 0.4238, 0.3847, 0.4633, 0.4075, 0.4236, 0.4044, 0.4641,\n",
       "         0.4041, 0.4293, 0.3886, 0.4763, 0.4157, 0.4264, 0.4137, 0.4628, 0.4041,\n",
       "         0.4247, 0.3829, 0.4755, 0.4104, 0.4304, 0.3985, 0.4644, 0.4071, 0.4320,\n",
       "         0.3890, 0.4779, 0.4178, 0.4280, 0.4147, 0.4634, 0.4173, 0.4255, 0.3849,\n",
       "         0.4684, 0.4118, 0.4241, 0.4049, 0.4597, 0.4030, 0.4327, 0.3871, 0.4687,\n",
       "         0.4088, 0.4251, 0.4079, 0.4682, 0.4203, 0.4257, 0.4043, 0.4674, 0.4186,\n",
       "         0.4271, 0.4067, 0.4645, 0.4118, 0.4207, 0.3822, 0.4729, 0.4076, 0.4298,\n",
       "         0.4152, 0.4631, 0.4188, 0.4250, 0.3871, 0.4767, 0.4087, 0.4333, 0.4099,\n",
       "         0.4549, 0.4133, 0.4221, 0.3875, 0.4748, 0.3976, 0.4306, 0.3984, 0.4638,\n",
       "         0.4225, 0.4257, 0.3919, 0.4711, 0.4172, 0.4244, 0.4040, 0.4554, 0.3943,\n",
       "         0.4298, 0.3835, 0.4729, 0.4069, 0.4283, 0.4121, 0.4638, 0.4140, 0.4279,\n",
       "         0.3925, 0.4801, 0.4329, 0.4220, 0.4106, 0.4553, 0.4122, 0.4261, 0.4003,\n",
       "         0.4692, 0.4116, 0.4297, 0.3968, 0.4631, 0.4056, 0.4319, 0.3991, 0.4682,\n",
       "         0.4127, 0.4233, 0.4122, 0.4635, 0.4070, 0.4275, 0.3924, 0.4755, 0.4156,\n",
       "         0.4231, 0.4021, 0.4675, 0.4164, 0.4294, 0.4055, 0.4748, 0.4207, 0.4290,\n",
       "         0.4037, 0.4598, 0.4168, 0.4194, 0.4002, 0.4677, 0.4080, 0.4281, 0.4015,\n",
       "         0.4679, 0.4192, 0.4283, 0.3968, 0.4688, 0.4091, 0.4288, 0.4162, 0.4591,\n",
       "         0.4098, 0.4233, 0.3821, 0.4710, 0.4022, 0.4296, 0.4113, 0.4688, 0.4218,\n",
       "         0.4224, 0.3980, 0.4735, 0.4170, 0.4295, 0.4133, 0.4626, 0.4101, 0.4216,\n",
       "         0.3806, 0.4804, 0.4145, 0.4291, 0.4151, 0.4673, 0.4165, 0.4272, 0.3803,\n",
       "         0.4811, 0.4280, 0.4235, 0.4184, 0.4632, 0.4129, 0.4265, 0.3782, 0.4738,\n",
       "         0.4035, 0.4276, 0.4059, 0.4631, 0.4240, 0.4241, 0.3925, 0.4685, 0.4106,\n",
       "         0.4248, 0.4163, 0.4612, 0.4063, 0.4272, 0.4031, 0.4784, 0.4219, 0.4273,\n",
       "         0.3907, 0.4642, 0.3807]], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formfactor = torch.rand(3, 240)\n",
    "rf_settings = torch.rand(3, 5)\n",
    "bunch_length = torch.rand(3, 1)\n",
    "\n",
    "current_profile = generator(formfactor, rf_settings, bunch_length)\n",
    "current_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Critic(\n",
       "  (formfactor_encoder): SignalEncoder(\n",
       "    (convnet): Sequential(\n",
       "      (0): Conv1d(1, 8, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Conv1d(8, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Conv1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=960, out_features=100, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=50, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (current_encoder): SignalEncoder(\n",
       "    (convnet): Sequential(\n",
       "      (0): Conv1d(1, 8, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Conv1d(8, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Conv1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=1216, out_features=100, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=50, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=26, out_features=50, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=50, out_features=20, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic = Critic()\n",
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1022],\n",
       "        [-0.1214],\n",
       "        [-0.1093]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic(current_profile, formfactor, rf_settings, bunch_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spectral-vd-euxfel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
