{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from train_gan import (\n",
    "    Critic,\n",
    "    EuXFELCurrentDataModule,\n",
    "    EuXFELCurrentDataset,\n",
    "    Generator,\n",
    "    SignalDecoder,\n",
    "    SignalEncoder,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignalEncoder(\n",
       "  (convnet): Sequential(\n",
       "    (0): Conv1d(1, 8, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Conv1d(8, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Conv1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=960, out_features=100, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formfactor_encoder = SignalEncoder(signal_dims=240, latent_dims=10)\n",
    "formfactor_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER signal.size() = torch.Size([3, 240]) signal.requires_grad = False\n",
      "ENCODER after torch.unsqueeze(signal, dim=1) x.size() = torch.Size([3, 1, 240]) x.requires_grad = False\n",
      "ENCODER EXPECT GRAD NEXT\n",
      "ENCODER after self.convnet(x) x.size() = torch.Size([3, 32, 30]) x.requires_grad = True\n",
      "ENCODER after self.flatten(x) x.size() = torch.Size([3, 960]) x.requires_grad = True\n",
      "ENCODER encoded.size() = torch.Size([3, 10]) encoded.requires_grad = True\n",
      "encoded = tensor([[ 0.1248,  0.0462, -0.0246,  0.0263,  0.0782, -0.0171,  0.0932, -0.1210,\n",
      "          0.0595,  0.0265],\n",
      "        [ 0.1248,  0.0478, -0.0253,  0.0280,  0.0787, -0.0158,  0.0945, -0.1197,\n",
      "          0.0577,  0.0270],\n",
      "        [ 0.1218,  0.0465, -0.0240,  0.0256,  0.0807, -0.0161,  0.0943, -0.1180,\n",
      "          0.0617,  0.0255]], grad_fn=<AddmmBackward0>)\n",
      "encoded.requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "formfactor = torch.rand(3, 240)\n",
    "encoded = formfactor_encoder(formfactor)\n",
    "print(f\"{encoded = }\")\n",
    "print(f\"{encoded.requires_grad = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignalEncoder(\n",
       "  (convnet): Sequential(\n",
       "    (0): Conv1d(1, 8, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Conv1d(8, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Conv1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=1216, out_features=100, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_encoder = SignalEncoder(signal_dims=300, latent_dims=10)\n",
    "current_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER signal.size() = torch.Size([3, 300]) signal.requires_grad = False\n",
      "ENCODER after torch.unsqueeze(signal, dim=1) x.size() = torch.Size([3, 1, 300]) x.requires_grad = False\n",
      "ENCODER EXPECT GRAD NEXT\n",
      "ENCODER after self.convnet(x) x.size() = torch.Size([3, 32, 38]) x.requires_grad = True\n",
      "ENCODER after self.flatten(x) x.size() = torch.Size([3, 1216]) x.requires_grad = True\n",
      "ENCODER encoded.size() = torch.Size([3, 10]) encoded.requires_grad = True\n",
      "encoded = tensor([[-0.0398, -0.0358,  0.0460,  0.0621,  0.1480,  0.0769, -0.0201,  0.1396,\n",
      "         -0.0206,  0.0531],\n",
      "        [-0.0433, -0.0331,  0.0438,  0.0589,  0.1445,  0.0784, -0.0195,  0.1412,\n",
      "         -0.0218,  0.0506],\n",
      "        [-0.0435, -0.0337,  0.0459,  0.0602,  0.1449,  0.0767, -0.0193,  0.1397,\n",
      "         -0.0244,  0.0504]], grad_fn=<AddmmBackward0>)\n",
      "encoded.requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "current_profile = torch.rand(3, 300)\n",
    "encoded = current_encoder(current_profile)\n",
    "print(f\"{encoded = }\")\n",
    "print(f\"{encoded.requires_grad = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignalDecoder(\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=50, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=50, out_features=100, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=100, out_features=1216, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (unflatten): Unflatten(dim=1, unflattened_size=(32, 38))\n",
       "  (convnet): Sequential(\n",
       "    (0): ConvTranspose1d(32, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): ConvTranspose1d(16, 8, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): ConvTranspose1d(8, 1, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "    (5): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_decoder = SignalDecoder(latent_dims=10 + 5 + 1, signal_dims=300)\n",
    "current_decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_decoder.parameters().__next__().requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIGNAL DECODER encoded.size() = torch.Size([3, 16]) encoded.requires_grad = False\n",
      "SIGNAL DECODER EXPECT GRAD NEXT\n",
      "SIGNAL DECODER x.size() = torch.Size([3, 1216]) x.requires_grad = True\n",
      "SIGNAL DECODER x.size() = torch.Size([3, 32, 38]) x.requires_grad = True\n",
      "SIGNAL DECODER x.size() = torch.Size([3, 1, 300]) x.requires_grad = True\n",
      "SIGNAL DECODER signal.size() = torch.Size([3, 300]) signal.requires_grad = True\n",
      "decoded = tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "decoded.requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "latent = torch.rand(3, 10 + 5 + 1)\n",
    "decoded = current_decoder(latent)\n",
    "print(f\"{decoded = }\")\n",
    "print(f\"{decoded.requires_grad = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignalDecoder(\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=50, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=50, out_features=100, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=100, out_features=960, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (unflatten): Unflatten(dim=1, unflattened_size=(32, 30))\n",
       "  (convnet): Sequential(\n",
       "    (0): ConvTranspose1d(32, 16, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): ConvTranspose1d(16, 8, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): ConvTranspose1d(8, 1, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "    (5): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formfactor_decoder = SignalDecoder(latent_dims=10 + 5 + 1, signal_dims=240)\n",
    "formfactor_decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIGNAL DECODER encoded.size() = torch.Size([3, 16]) encoded.requires_grad = False\n",
      "SIGNAL DECODER EXPECT GRAD NEXT\n",
      "SIGNAL DECODER x.size() = torch.Size([3, 960]) x.requires_grad = True\n",
      "SIGNAL DECODER x.size() = torch.Size([3, 32, 30]) x.requires_grad = True\n",
      "SIGNAL DECODER x.size() = torch.Size([3, 1, 240]) x.requires_grad = True\n",
      "SIGNAL DECODER signal.size() = torch.Size([3, 240]) signal.requires_grad = True\n",
      "decoded = tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "decoded.requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "latent = torch.rand(3, 10 + 5 + 1)\n",
    "decoded = formfactor_decoder(latent)\n",
    "print(f\"{decoded = }\")\n",
    "print(f\"{decoded.requires_grad = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (formfactor_encoder): SignalEncoder(\n",
       "    (convnet): Sequential(\n",
       "      (0): Conv1d(1, 8, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Conv1d(8, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Conv1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=960, out_features=100, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=50, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (current_decoder): SignalDecoder(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=50, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=50, out_features=100, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=100, out_features=1216, bias=True)\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (unflatten): Unflatten(dim=1, unflattened_size=(32, 38))\n",
       "    (convnet): Sequential(\n",
       "      (0): ConvTranspose1d(32, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): ConvTranspose1d(16, 8, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): ConvTranspose1d(8, 1, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator()\n",
    "generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.parameters().__next__().requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATOR formfactor.size() = torch.Size([3, 240]) formfactor.requires_grad = False\n",
      "GENERATOR rf_settings.size() = torch.Size([3, 5]) rf_settings.requires_grad = False\n",
      "GENERATOR bunch_length.size() = torch.Size([3, 1]) bunch_length.requires_grad = False\n",
      "GENERATOR‚ EXPECT GRAD NEXT\n",
      "ENCODER signal.size() = torch.Size([3, 240]) signal.requires_grad = False\n",
      "ENCODER after torch.unsqueeze(signal, dim=1) x.size() = torch.Size([3, 1, 240]) x.requires_grad = False\n",
      "ENCODER EXPECT GRAD NEXT\n",
      "ENCODER after self.convnet(x) x.size() = torch.Size([3, 32, 30]) x.requires_grad = True\n",
      "ENCODER after self.flatten(x) x.size() = torch.Size([3, 960]) x.requires_grad = True\n",
      "ENCODER encoded.size() = torch.Size([3, 10]) encoded.requires_grad = True\n",
      "GENERATOR encoded_formfactor.size() = torch.Size([3, 10]) encoded_formfactor.requires_grad = True\n",
      "GENERATOR latent.size() = torch.Size([3, 16]) latent.requires_grad = True\n",
      "SIGNAL DECODER encoded.size() = torch.Size([3, 16]) encoded.requires_grad = True\n",
      "SIGNAL DECODER EXPECT GRAD NEXT\n",
      "SIGNAL DECODER x.size() = torch.Size([3, 1216]) x.requires_grad = True\n",
      "SIGNAL DECODER x.size() = torch.Size([3, 32, 38]) x.requires_grad = True\n",
      "SIGNAL DECODER x.size() = torch.Size([3, 1, 300]) x.requires_grad = True\n",
      "SIGNAL DECODER signal.size() = torch.Size([3, 300]) signal.requires_grad = True\n",
      "GENERATOR current.size() = torch.Size([3, 300]) current.requires_grad = True\n",
      "current_profile.size() = torch.Size([3, 300])\n",
      "current_profile.requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "formfactor = torch.rand(3, 240)\n",
    "rf_settings = torch.rand(3, 5)\n",
    "bunch_length = torch.rand(3, 1)\n",
    "\n",
    "current_profile = generator(formfactor, rf_settings, bunch_length)\n",
    "print(f\"{current_profile.size() = }\")\n",
    "print(f\"{current_profile.requires_grad = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Critic(\n",
       "  (formfactor_encoder): SignalEncoder(\n",
       "    (convnet): Sequential(\n",
       "      (0): Conv1d(1, 8, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Conv1d(8, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Conv1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=960, out_features=100, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=50, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (current_encoder): SignalEncoder(\n",
       "    (convnet): Sequential(\n",
       "      (0): Conv1d(1, 8, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Conv1d(8, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Conv1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=1216, out_features=100, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=50, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=26, out_features=50, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=50, out_features=20, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic = Critic()\n",
    "critic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER signal.size() = torch.Size([3, 300]) signal.requires_grad = True\n",
      "ENCODER after torch.unsqueeze(signal, dim=1) x.size() = torch.Size([3, 1, 300]) x.requires_grad = True\n",
      "ENCODER EXPECT GRAD NEXT\n",
      "ENCODER after self.convnet(x) x.size() = torch.Size([3, 32, 38]) x.requires_grad = True\n",
      "ENCODER after self.flatten(x) x.size() = torch.Size([3, 1216]) x.requires_grad = True\n",
      "ENCODER encoded.size() = torch.Size([3, 10]) encoded.requires_grad = True\n",
      "ENCODER signal.size() = torch.Size([3, 240]) signal.requires_grad = False\n",
      "ENCODER after torch.unsqueeze(signal, dim=1) x.size() = torch.Size([3, 1, 240]) x.requires_grad = False\n",
      "ENCODER EXPECT GRAD NEXT\n",
      "ENCODER after self.convnet(x) x.size() = torch.Size([3, 32, 30]) x.requires_grad = True\n",
      "ENCODER after self.flatten(x) x.size() = torch.Size([3, 960]) x.requires_grad = True\n",
      "ENCODER encoded.size() = torch.Size([3, 10]) encoded.requires_grad = True\n",
      "CRITIC (out) x.requires_grad = True\n",
      "critique = tensor([[0.0519],\n",
      "        [0.0489],\n",
      "        [0.0452]], grad_fn=<AddmmBackward0>)\n",
      "critique.requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "critique = critic(current_profile, formfactor, rf_settings, bunch_length)\n",
    "print(f\"{critique = }\")\n",
    "print(f\"{critique.requires_grad = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<train_gan.EuXFELCurrentDataset at 0x15ff700a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = EuXFELCurrentDataset()\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(formfactor, rf_settings, bunch_length), current_profile = dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0044,  0.9630,  1.0036,  0.8943,  0.9871,  1.2385,  1.0386,  1.0309,\n",
       "         0.9027,  1.0560,  0.9660,  0.9919,  0.9544,  1.0035,  0.9802,  0.9358,\n",
       "         0.9007,  0.9520,  0.9279,  0.9389,  0.9560,  0.9737,  0.9385,  0.9257,\n",
       "         0.9457,  0.9194,  0.9217,  0.9189,  0.8836,  0.9069,  0.8627,  0.9422,\n",
       "         0.8833,  0.8639,  0.9203,  0.8686,  0.9047,  0.8307,  0.8406,  0.8705,\n",
       "         0.8725,  0.8646,  0.8225,  0.8547,  0.8434,  0.8378,  0.8149,  0.8217,\n",
       "         0.8124,  0.7939,  0.7970,  0.7831,  0.7742,  0.7711,  0.7550,  0.7392,\n",
       "         0.7150,  0.7093,  0.6877,  0.6594,  0.7884,  0.6811,  0.6007,  0.5821,\n",
       "         0.6001,  0.6237,  0.5887,  0.5902,  0.6090,  0.5914,  0.5975,  0.5878,\n",
       "         0.5588,  0.5395,  0.5357,  0.5267,  0.5074,  0.4935,  0.4823,  0.4565,\n",
       "         0.4283,  0.4146,  0.4020,  0.3654,  0.3516,  0.3306,  0.2884,  0.2757,\n",
       "         0.2254,  0.2019,  0.2199,  0.3086,  0.2065,  0.2649,  0.2790,  0.2603,\n",
       "         0.1840,  0.1837,  0.1600,  0.1796,  0.1674,  0.1718,  0.1225,  0.1754,\n",
       "         0.1596,  0.1512,  0.1817,  0.1655,  0.1556,  0.1630,  0.1643,  0.1622,\n",
       "         0.1689,  0.1703,  0.1824,  0.1843,  0.1869,  0.1950,  0.1856,  0.1793,\n",
       "         0.1315,  0.1815,  0.1174,  0.1649,  0.1652,  0.1422,  0.1521,  0.1646,\n",
       "         0.1159,  0.1494,  0.1368,  0.1348,  0.0939,  0.1110,  0.0894, -0.1676,\n",
       "         0.0948,  0.0842,  0.0787,  0.0809,  0.0831,  0.0949,  0.1049,  0.1108,\n",
       "         0.1188,  0.1211,  0.1232,  0.1102,  0.1028,  0.1504,  0.1250, -0.0604,\n",
       "         0.0946,  0.0669,  0.0399,  0.1073,  0.0760,  0.0626,  0.0991,  0.0810,\n",
       "         0.0327,  0.0625,  0.0732,  0.0690,  0.0524,  0.0518,  0.0711,  0.1159,\n",
       "         0.1057,  0.0843,  0.0875,  0.0817,  0.0755,  0.0621,  0.0452,  0.0378,\n",
       "         0.0482,  0.0556,  0.0646,  0.0556,  0.1044,  0.0345,  0.0352, -0.0057,\n",
       "         0.0340,  0.0368, -0.0286, -0.0291, -0.0331, -0.0829,  0.0418,  0.0506,\n",
       "         0.0554,  0.0530,  0.0522,  0.0417,  0.0495,  0.0246,  0.0179,  0.0335,\n",
       "         0.0385,  0.0402,  0.0385,  0.0277,  0.0138,  0.0280,  0.0381,  0.0349,\n",
       "         0.0035,  0.0736,  0.0598, -0.0373,  0.0310, -0.0488,  0.0804,  0.0565,\n",
       "        -0.0353,  0.0429,  0.0167,  0.0282,  0.0257,  0.0180,  0.0116,  0.0398,\n",
       "         0.0453,  0.0231,  0.0190,  0.0393, -0.0196, -0.0311, -0.0467,  0.0197,\n",
       "        -0.0309, -0.0333,  0.0126, -0.0147,  0.0204,  0.0125, -0.0295,  0.0559])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formfactor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.4400e+00,  2.8130e+02,  4.5442e+04, -1.3430e+01,  2.1100e+00])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.0783e-05)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunch_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 179.1833,  265.1158,  370.1751,  491.9395,  626.4266,  768.1013,\n",
       "         908.7740, 1042.1052, 1162.9528, 1264.8722, 1348.4093, 1413.8678,\n",
       "        1462.4076, 1493.7413, 1512.1744, 1519.9905, 1519.4473, 1512.3607,\n",
       "        1502.1366, 1490.9547, 1480.4468, 1471.3280, 1463.6165, 1456.7324,\n",
       "        1449.9124, 1442.8661, 1435.5945, 1428.8264, 1423.0869, 1419.1271,\n",
       "        1416.9052, 1415.7997, 1415.0654, 1413.8822, 1411.8910, 1409.0577,\n",
       "        1405.5623, 1401.7904, 1397.8761, 1393.8970, 1390.1157, 1387.0282,\n",
       "        1384.8540, 1383.8727, 1383.8990, 1384.2417, 1383.5660, 1380.6753,\n",
       "        1374.9377, 1366.2657, 1356.0144, 1346.1371, 1338.5245, 1334.9456,\n",
       "        1335.8098, 1340.0797, 1346.2191, 1352.3209, 1356.5610, 1357.8591,\n",
       "        1355.5807, 1349.4393, 1340.0415, 1328.5688, 1316.5232, 1305.4174,\n",
       "        1296.7922, 1291.0576, 1288.1896, 1287.8152, 1289.4836, 1292.4856,\n",
       "        1296.4257, 1300.7830, 1304.6067, 1307.0728, 1307.5531, 1305.6469,\n",
       "        1301.3713, 1295.5537, 1288.7943, 1281.6586, 1274.7197, 1268.8696,\n",
       "        1264.6313, 1262.5005, 1262.1154, 1262.1019, 1260.9780, 1257.2867,\n",
       "        1249.9773, 1240.0989, 1229.0289, 1218.6681, 1211.3458, 1207.7805,\n",
       "        1208.2054, 1211.8888, 1217.3285, 1221.9727, 1224.0422, 1222.2666,\n",
       "        1215.5594, 1205.0040, 1191.6608, 1176.6780, 1161.4698, 1147.1185,\n",
       "        1134.6243, 1124.8486, 1118.9165, 1116.4080, 1116.8114, 1119.0972,\n",
       "        1122.1719, 1125.0762, 1127.3248, 1128.8667, 1129.5229, 1129.1501,\n",
       "        1127.1558, 1122.9415, 1115.7234, 1106.2776, 1095.7361, 1085.6208,\n",
       "        1078.0215, 1074.0427, 1073.8761, 1076.9758, 1082.3121, 1088.0555,\n",
       "        1092.9122, 1096.0947, 1096.9550, 1095.6292, 1092.6721, 1088.3611,\n",
       "        1082.8762, 1076.2614, 1068.3983, 1059.0015, 1047.6423, 1034.3521,\n",
       "        1019.6658, 1004.2232,  989.0231,  975.2514,  963.6213,  954.6097,\n",
       "         948.3491,  944.3639,  941.6473,  939.3870,  936.7869,  933.2231,\n",
       "         928.5677,  922.8477,  916.1462,  908.6627,  900.9128,  893.1390,\n",
       "         885.6578,  878.5041,  871.5567,  864.5103,  856.9824,  848.5487,\n",
       "         839.1793,  828.8627,  817.7148,  805.8785,  793.7277,  781.6601,\n",
       "         770.1476,  759.9048,  751.2829,  744.6625,  740.2706,  738.4003,\n",
       "         738.3168,  739.6046,  741.6005,  743.5164,  744.5435,  744.0992,\n",
       "         741.6838,  736.8547,  729.8548,  720.9260,  710.4181,  698.9488,\n",
       "         687.4023,  676.4956,  666.9177,  659.2346,  653.3217,  649.0088,\n",
       "         645.9459,  643.7675,  641.9937,  640.2684,  638.2169,  635.5790,\n",
       "         632.2490,  628.2607,  623.6092,  618.4365,  612.9874,  607.4666,\n",
       "         602.0009,  596.7210,  591.7306,  587.1321,  582.8207,  579.0054,\n",
       "         575.5805,  572.5874,  570.0806,  568.2402,  567.1962,  567.1440,\n",
       "         568.3934,  571.3211,  576.2237,  583.0677,  591.7641,  601.7861,\n",
       "         611.9141,  620.5814,  625.8419,  624.6522,  614.5303,  593.7863,\n",
       "         561.1049,  514.9560,  457.5943,  392.8271,  324.7126,  258.2587,\n",
       "         198.0663,  146.5870,  105.0448,   74.1945,   52.3957,   37.3627,\n",
       "          27.4340,   21.3620,   17.7514,   15.5304,   14.1450,   13.1850,\n",
       "          12.4636,   11.8231,   11.2314,   10.6607,   10.1095,    9.5635,\n",
       "           9.0194,    8.4614,    7.8842,    7.2767,    6.6262,    5.9659,\n",
       "           5.3574,    4.8523,    4.4884,    4.3060,    4.2625,    4.2711,\n",
       "           4.2769,    4.2555,    4.1793,    4.0757,    3.9685,    3.8736,\n",
       "           3.7917,    3.7116,    3.6248,    3.5051,    3.3435,    3.1277,\n",
       "           2.8783,    2.6425,    2.4862,    2.4393,    2.5071,    2.6408,\n",
       "           2.7782,    2.8578,    2.8474,    2.7545,    2.6347,    2.5309,\n",
       "           2.4717,    2.4547,    2.4519,    2.4130,    2.2959,    2.0813])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<train_gan.EuXFELCurrentDataModule at 0x15ff700d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = EuXFELCurrentDataModule(batch_size=64)\n",
    "data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.setup(stage=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x15ff705e0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = data_module.train_dataloader()\n",
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spectral-vd-euxfel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
